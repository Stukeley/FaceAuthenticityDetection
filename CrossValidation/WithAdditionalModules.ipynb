{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T13:53:53.155204Z",
     "start_time": "2025-04-19T13:53:53.132770Z"
    }
   },
   "source": [
    "# Najpierw trzeba sprawdzić wyniki z pliku ImageResults -> jaki jest accuracy\n",
    "# W każdej linii - pierwszy znak T lub F oznacza oczekiwany rezultat, a ostatnia wartość (authentic/spoof) - uzyskany\n",
    "THRESHOLD = 0.50"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T13:53:53.209127Z",
     "start_time": "2025-04-19T13:53:53.175901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = \"ImageResults.txt\"\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate through each line in the file\n",
    "for line in lines:\n",
    "    # Split the line into parts\n",
    "    parts = line.strip().split(\" \")\n",
    "\n",
    "    expected_result = line[0]\n",
    "    obtained_result = parts[-1].strip()  # Obtained result (authentic/spoof)\n",
    "\n",
    "    # Check if the expected result matches the obtained result\n",
    "    if expected_result == \"T\" and obtained_result == \"authentic\":\n",
    "        correct_predictions += 1\n",
    "    elif expected_result == \"F\" and obtained_result == \"spoof\":\n",
    "        correct_predictions += 1\n",
    "\n",
    "    total_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "# TO JEST Z 55 threshold"
   ],
   "id": "8f09987ebaa7d010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.77%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T14:19:12.169204Z",
     "start_time": "2025-04-19T14:19:12.049271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wczytanie danych aby puścić już wytrenowany model\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "auth_path = \"/Users/stukeleyak/Desktop/Studia/Doktorat/Projekt/FaceAuthenticityDetection/TestyInnychSystemów/PyTorchModelsTests/dane/CelebA/authentic\"\n",
    "spoof_path = \"/Users/stukeleyak/Desktop/Studia/Doktorat/Projekt/FaceAuthenticityDetection/TestyInnychSystemów/PyTorchModelsTests/dane/CelebA/spoof\"\n",
    "\n",
    "auth_images = [os.path.join(auth_path, img) for img in os.listdir(auth_path)]\n",
    "spoof_images = [os.path.join(spoof_path, img) for img in os.listdir(spoof_path)]\n",
    "\n",
    "print(\"Loaded images:\")\n",
    "print(f\"Authentic: {len(auth_images)}\")\n",
    "print(f\"Spoof: {len(spoof_images)}\")"
   ],
   "id": "2eeb82581ca150ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images:\n",
      "Authentic: 13543\n",
      "Spoof: 9509\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T14:31:24.005445Z",
     "start_time": "2025-04-19T14:24:15.346494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wyniki dla modelu ConvNeXt\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "model = models.convnext_small(weights=None)\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_ftrs, 2)  # Two output neurons\n",
    "\n",
    "# Load the model weights\n",
    "path = \"/Users/stukeleyak/Desktop/Studia/Doktorat/Projekt/FaceAuthenticityDetection/CrossWalidacja/best_model_convnext.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "# Transform the images\n",
    "# Create a transform to resize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, image_path  # Return image, label, and file path\n",
    "\n",
    "\n",
    "# Create custom datasets\n",
    "test_dataset = OurDataset(auth_images + spoof_images, [0] * len(auth_images) + [1] * len(spoof_images), transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a text file to store incorrect predictions\n",
    "with open(\"incorrect_predictions_convnext.txt\", \"w\") as f:\n",
    "    f.write(\"name\\tpredicted\\tactual\\n\")\n",
    "    f.write(\"(0 - authentic, 1 - spoof)\\n\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for images, labels, image_paths in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get the argmax of each output\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # TODO: aktualnie predykcje sa tylko 0/1, moze trzeba zmienic na prawdopodobienstwa softmaxem\n",
    "\n",
    "    total_predictions += labels.size(0)\n",
    "\n",
    "    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Write all predictions to the file\n",
    "    with open(\"incorrect_predictions_convnext.txt\", \"a\") as f:\n",
    "        for i in range(len(predicted)):\n",
    "            f.write(f\"{image_paths[i]}\\t{predicted[i].item()}\\t{labels[i].item()}\\n\")\n",
    "\n",
    "print(\"Accuracy for ConvNeXt model:\")\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "28653b557343a85c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/2402830029.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ConvNeXt model:\n",
      "Accuracy: 94.52%\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T14:46:18.401013Z",
     "start_time": "2025-04-19T14:46:15.403281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for the final NN\n",
    "# [M1, M2, M3, M4, M5, Expected]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create two maps: (name -> M5) and (name -> M1..M4)\n",
    "# Then create rows like [M1, M2, ..., M5, expected]\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data_dict = {}\n",
    "new_data_dict = {}\n",
    "\n",
    "df = pd.DataFrame(columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"Expected\"])\n",
    "\n",
    "# Read the file and populate the dictionary\n",
    "with open(\"ImageResults.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[1:]  # Skip the first line\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\" \")\n",
    "        name = parts[0]\n",
    "        m1 = float(parts[1])\n",
    "        m2 = float(parts[2])\n",
    "        m3 = float(parts[3])\n",
    "        m4 = float(parts[4])\n",
    "\n",
    "        new_data_dict[name] = [m1, m2, m3, m4]\n",
    "\n",
    "# Read second file\n",
    "with open(\"incorrect_predictions_convnext.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in new_data_dict:\n",
    "            m1m4 = new_data_dict[name]\n",
    "            m1 = m1m4[0]\n",
    "            m2 = m1m4[1]\n",
    "            m3 = m1m4[2]\n",
    "            m4 = m1m4[3]\n",
    "\n",
    "            row = [m1, m2, m3, m4, predicted, expected]\n",
    "\n",
    "            # Add row to df\n",
    "            df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"Expected\"])], ignore_index=True)\n",
    "\n",
    "print(\"Dataframe length:\")\n",
    "print(len(df))"
   ],
   "id": "e98818424774d951",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/3310867508.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"Expected\"])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe length:\n",
      "22324\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T14:49:03.713474Z",
     "start_time": "2025-04-19T14:49:03.710119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final NN\n",
    "# Parameter to control the amount of inputs to the NN\n",
    "N_INPUTS = 5\n",
    "\n",
    "class ProbabilityNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbabilityNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_INPUTS, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "id": "3334de377411f3a9",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T14:50:23.527975Z",
     "start_time": "2025-04-19T14:50:10.156291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_probability_nn(df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    model = ProbabilityNN()\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_probability_nn(df)"
   ],
   "id": "18c1330437e1b994",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T15:05:48.314786Z",
     "start_time": "2025-04-19T15:05:47.931574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check accuracy\n",
    "def check_accuracy(model, df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            y_pred = model(x)\n",
    "            predictions = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
    "            correct_predictions += (predictions == y).sum().item()\n",
    "            total_predictions += y.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = check_accuracy(model, df)\n",
    "\n",
    "print(f\"Final NN accuracy: {accuracy:.2f}%\")"
   ],
   "id": "dacd3144736ba81d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NN accuracy: 99.24%\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:57:49.137823Z",
     "start_time": "2025-04-20T16:34:21.945349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now add EfficientNetV2 and do the same\n",
    "# Wyniki dla modelu EfficientNetV2\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "model = models.efficientnet_v2_s(weights=None)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)  # Two output neurons\n",
    "\n",
    "# Load the model weights\n",
    "path = \"/Users/stukeleyak/Desktop/Studia/Doktorat/Projekt/FaceAuthenticityDetection/CrossWalidacja/best_efficientnetv2_model.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "# Transform the images\n",
    "# Create a transform to resize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, image_path  # Return image, label, and file path\n",
    "\n",
    "\n",
    "# Create custom datasets\n",
    "test_dataset = OurDataset(auth_images + spoof_images, [0] * len(auth_images) + [1] * len(spoof_images), transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a text file to store incorrect predictions\n",
    "with open(\"incorrect_predictions_efficientnet.txt\", \"w\") as f:\n",
    "    f.write(\"name\\tpredicted\\tactual\\n\")\n",
    "    f.write(\"(0 - authentic, 1 - spoof)\\n\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for images, labels, image_paths in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get the argmax of each output\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    total_predictions += labels.size(0)\n",
    "\n",
    "    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Write all predictions to the file\n",
    "    with open(\"incorrect_predictions_efficientnet.txt\", \"a\") as f:\n",
    "        for i in range(len(predicted)):\n",
    "            f.write(f\"{image_paths[i]}\\t{predicted[i].item()}\\t{labels[i].item()}\\n\")\n",
    "\n",
    "print(\"Accuracy for EfficientNetV2 model:\")\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "8da95009ceb5fb18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/3850916976.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for EfficientNetV2 model:\n",
      "Accuracy: 95.16%\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:57:51.954599Z",
     "start_time": "2025-04-20T16:57:49.180762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for the final NN\n",
    "# [M1, M2, M3, M4, M5, M6, Expected]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data_dict = {}\n",
    "new_data_dict = {}\n",
    "second_data_dict = {}\n",
    "\n",
    "df = pd.DataFrame(columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"Expected\"])\n",
    "\n",
    "# Read the file and populate the dictionary\n",
    "with open(\"ImageResults.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[1:]  # Skip the first line\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\" \")\n",
    "        name = parts[0]\n",
    "        m1 = float(parts[1])\n",
    "        m2 = float(parts[2])\n",
    "        m3 = float(parts[3])\n",
    "        m4 = float(parts[4])\n",
    "\n",
    "        new_data_dict[name] = [m1, m2, m3, m4]\n",
    "\n",
    "# Read second file\n",
    "with open(\"incorrect_predictions_convnext.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in new_data_dict:\n",
    "            m1m4 = new_data_dict[name]\n",
    "            m1 = m1m4[0]\n",
    "            m2 = m1m4[1]\n",
    "            m3 = m1m4[2]\n",
    "            m4 = m1m4[3]\n",
    "\n",
    "            row = [m1, m2, m3, m4, predicted]\n",
    "\n",
    "            second_data_dict[name] = row\n",
    "\n",
    "with open(\"incorrect_predictions_efficientnet.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in second_data_dict:\n",
    "            m1m5 = second_data_dict[name]\n",
    "            m1 = m1m5[0]\n",
    "            m2 = m1m5[1]\n",
    "            m3 = m1m5[2]\n",
    "            m4 = m1m5[3]\n",
    "            m5 = m1m5[4]\n",
    "\n",
    "            row = [m1, m2, m3, m4, m5, predicted, expected]\n",
    "\n",
    "            # Add row to df\n",
    "            df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"Expected\"])], ignore_index=True)\n",
    "\n",
    "print(\"Dataframe length:\")\n",
    "print(len(df))"
   ],
   "id": "8821ed6953c76dbb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/2616724625.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"Expected\"])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe length:\n",
      "22324\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:57:51.970332Z",
     "start_time": "2025-04-20T16:57:51.968253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final NN\n",
    "# Parameter to control the amount of inputs to the NN\n",
    "N_INPUTS = 6\n",
    "\n",
    "class ProbabilityNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbabilityNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_INPUTS, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "id": "e277034e383f1a30",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:58:04.455529Z",
     "start_time": "2025-04-20T16:57:52.001285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_probability_nn(df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    model = ProbabilityNN()\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_probability_nn(df)"
   ],
   "id": "99f54d413a1dbf69",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:03:01.272622Z",
     "start_time": "2025-04-20T17:03:00.888501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check accuracy\n",
    "def check_accuracy(model, df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            y_pred = model(x)\n",
    "            predictions = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
    "            correct_predictions += (predictions == y).sum().item()\n",
    "            total_predictions += y.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = check_accuracy(model, df)\n",
    "\n",
    "print(f\"Final NN accuracy: {accuracy:.2f}%\")"
   ],
   "id": "b3d5dc4e66194415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NN accuracy: 99.29%\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:24.802515Z",
     "start_time": "2025-04-20T17:06:52.957920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now add Vision Transformer and do the same\n",
    "# Wyniki dla modelu ViT\n",
    "model = models.vit_b_16(weights=None)   # b_32 for performance, b_16 might be better\n",
    "num_ftrs = model.heads.head.in_features\n",
    "model.heads.head = torch.nn.Linear(num_ftrs, 2) # Two output neurons\n",
    "\n",
    "# Load the model weights\n",
    "path = \"/Users/stukeleyak/Desktop/Studia/Doktorat/Projekt/FaceAuthenticityDetection/CrossWalidacja/best_model_visiontransformer.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "# Transform the images\n",
    "# Create a transform to resize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, image_path  # Return image, label, and file path\n",
    "\n",
    "\n",
    "# Create custom datasets\n",
    "test_dataset = OurDataset(auth_images + spoof_images, [0] * len(auth_images) + [1] * len(spoof_images), transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a text file to store incorrect predictions\n",
    "with open(\"incorrect_predictions_visiontransformer.txt\", \"w\") as f:\n",
    "    f.write(\"name\\tpredicted\\tactual\\n\")\n",
    "    f.write(\"(0 - authentic, 1 - spoof)\\n\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for images, labels, image_paths in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get the argmax of each output\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    total_predictions += labels.size(0)\n",
    "\n",
    "    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Write all predictions to the file\n",
    "    with open(\"incorrect_predictions_visiontransformer.txt\", \"a\") as f:\n",
    "        for i in range(len(predicted)):\n",
    "            f.write(f\"{image_paths[i]}\\t{predicted[i].item()}\\t{labels[i].item()}\\n\")\n",
    "\n",
    "print(\"Accuracy for ViT model:\")\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "91ec62a677deef94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/3974107435.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ViT model:\n",
      "Accuracy: 46.64%\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:28.356914Z",
     "start_time": "2025-04-20T17:30:24.995577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for the final NN\n",
    "# [M1, M2, M3, M4, M5, M6, Expected]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data_dict = {}\n",
    "new_data_dict = {}\n",
    "second_data_dict = {}\n",
    "third_data_dict = {}\n",
    "\n",
    "df = pd.DataFrame(columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"Expected\"])\n",
    "\n",
    "# Read the file and populate the dictionary\n",
    "with open(\"ImageResults.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[1:]  # Skip the first line\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\" \")\n",
    "        name = parts[0]\n",
    "        m1 = float(parts[1])\n",
    "        m2 = float(parts[2])\n",
    "        m3 = float(parts[3])\n",
    "        m4 = float(parts[4])\n",
    "\n",
    "        new_data_dict[name] = [m1, m2, m3, m4]\n",
    "\n",
    "# Read second file\n",
    "with open(\"incorrect_predictions_convnext.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in new_data_dict:\n",
    "            m1m4 = new_data_dict[name]\n",
    "            m1 = m1m4[0]\n",
    "            m2 = m1m4[1]\n",
    "            m3 = m1m4[2]\n",
    "            m4 = m1m4[3]\n",
    "\n",
    "            row = [m1, m2, m3, m4, predicted]\n",
    "\n",
    "            second_data_dict[name] = row\n",
    "\n",
    "with open(\"incorrect_predictions_efficientnet.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in second_data_dict:\n",
    "            m1m5 = second_data_dict[name]\n",
    "            m1 = m1m5[0]\n",
    "            m2 = m1m5[1]\n",
    "            m3 = m1m5[2]\n",
    "            m4 = m1m5[3]\n",
    "            m5 = m1m5[4]\n",
    "\n",
    "            row = [m1, m2, m3, m4, m5, predicted, expected]\n",
    "\n",
    "            third_data_dict[name] = row\n",
    "\n",
    "with open(\"incorrect_predictions_visiontransformer.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    lines = lines[2:]  # Skip the first two lines\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        name = parts[0]\n",
    "        name = name.split(\"/\")[-1]  # Get the file name only\n",
    "        predicted = float(parts[1])\n",
    "        expected = int(parts[2])\n",
    "\n",
    "        if name in third_data_dict:\n",
    "            m1m6 = third_data_dict[name]\n",
    "            m1 = m1m6[0]\n",
    "            m2 = m1m6[1]\n",
    "            m3 = m1m6[2]\n",
    "            m4 = m1m6[3]\n",
    "            m5 = m1m6[4]\n",
    "            m6 = m1m6[5]\n",
    "\n",
    "            row = [m1, m2, m3, m4, m5, m6, predicted, expected]\n",
    "\n",
    "            # Add row to df\n",
    "            df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"Expected\"])], ignore_index=True)\n",
    "\n",
    "print(\"Dataframe length:\")\n",
    "print(len(df))"
   ],
   "id": "180156a02bfb03b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/03wqtylj4nz_4lpvj_w6mc2h0000gn/T/ipykernel_19957/4166982489.py:102: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row], columns=[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"Expected\"])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe length:\n",
      "22324\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:28.371645Z",
     "start_time": "2025-04-20T17:30:28.368896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final NN\n",
    "# Parameter to control the amount of inputs to the NN\n",
    "N_INPUTS = 7\n",
    "\n",
    "class ProbabilityNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbabilityNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_INPUTS, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "id": "81a4f62935907d60",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:40.865780Z",
     "start_time": "2025-04-20T17:30:28.407118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_probability_nn(df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    model = ProbabilityNN()\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_probability_nn(df)"
   ],
   "id": "3ab16d1863fbb87",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:41.219433Z",
     "start_time": "2025-04-20T17:30:40.869417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check accuracy\n",
    "def check_accuracy(model, df):\n",
    "    inputs = df[[\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\"]]\n",
    "    outputs = df[[\"Expected\"]]\n",
    "    outputs = outputs.astype(np.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(inputs.values, dtype=torch.float32), torch.tensor(outputs.values, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y = y.view(-1, 1)\n",
    "            y_pred = model(x)\n",
    "            predictions = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
    "            correct_predictions += (predictions == y).sum().item()\n",
    "            total_predictions += y.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = check_accuracy(model, df)\n",
    "\n",
    "print(f\"Final NN accuracy: {accuracy:.2f}%\")"
   ],
   "id": "a8423c5dff195700",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NN accuracy: 99.34%\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
